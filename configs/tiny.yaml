org: ""
name: "tiny-100k"
group: ""
block_size: 8224
vocab_size: 1
padding_multiple: 1
padded_vocab_size: None
n_layer: 6
n_cot: 1
n_head:  16
n_embd: 256
intermediate_size: 1024
n_query_groups: 4
rotary_percentage: 1.0
parallel_residual: False
bias: False
_norm_class: "FusedRMSNorm"
norm_eps: 1e-5
_mlp_class: "LLaMAMLP"
shared_attention_norm: False
condense_ratio: 1
rope_base: 10000
patch_size: 32
forecasting_patch: 1
rolling_patch: 6
rollback_win: 256
scaling: True
quantitle: True
sum_divided: False
unet: True
ou: False
vq: False
imputation: False
new_arch: False
new_tokenizer: False
average_2: False
ou_mean: False
stats_encoding: False
stats_encoding_new: False
num_of_devices: 4
num_of_nodes: 1
global_batch_size:  512
micro_batch_size: 128
block_size_val: 2080
micro_batch_size_val:  512
max_step: 100_000
warmup_steps: 2000
learning_rate: 5e-4
weight_decay: 0.1
betas: (0.9,0.95)
grad_clip: 1.0
min_lr: 1e-5
log_step_interval: 10
eval_iters: 100
save_step_interval: 5000
eval_step_interval: 1000
seed0: 3407
inner_norm: False
ou_prod: False
discount: True
decay_lr: True
yj_trans: False
mix_train: False
haar_trans: True
haar_trans_inv: True
haar_trans_norm: "backward"
haar_loss_match: False
is_smape: False
is_diff: False
half_diff: False
pid: False
triple_diff: False
triple_diff_new: False
multi_loss: False
mean_replace: False
inter_control: False


